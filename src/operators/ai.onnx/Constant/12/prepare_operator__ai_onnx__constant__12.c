//this file was generated by ../../../../../../scripts/onnx_generator/OperatorTemplate.py
#include "operator__ai_onnx__constant__12.h"
#include "tracing.h"
#include "utils.h"
#include <string.h>

operator_status
prepare_operator__ai_onnx__constant__12(
    node_context *ctx
)
{
    TRACE_ENTRY(1);

    TRACE_NODE(2, true, ctx->onnx_node);

    /* UNCOMMENT AS NEEDED */

    // Onnx__AttributeProto *a_sparse_value = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"sparse_value");
    Onnx__AttributeProto *a_value = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"value");
    // Onnx__AttributeProto *a_value_float = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"value_float");
    // Onnx__AttributeProto *a_value_floats = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"value_floats");
    // Onnx__AttributeProto *a_value_int = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"value_int");
    // Onnx__AttributeProto *a_value_ints = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"value_ints");
    // Onnx__AttributeProto *a_value_string = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"value_string");
    // Onnx__AttributeProto *a_value_strings = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"value_strings");

    // TRACE_ATTRIBUTE(2, a_sparse_value, a_sparse_value);
    TRACE_ATTRIBUTE(2, a_value, a_value);
    // TRACE_ATTRIBUTE(2, a_value_float, a_value_float);
    // TRACE_ATTRIBUTE(2, a_value_floats, a_value_floats);
    // TRACE_ATTRIBUTE(2, a_value_int, a_value_int);
    // TRACE_ATTRIBUTE(2, a_value_ints, a_value_ints);
    // TRACE_ATTRIBUTE(2, a_value_string, a_value_string);
    // TRACE_ATTRIBUTE(2, a_value_strings, a_value_strings);

    Onnx__TensorProto *o_output = searchOutputByName(ctx, 0);

    /* ALLOCATE AND INITIALIZE CONTEXT HERE IF NEEDED */

    // Onnx__SparseTensorProto* default_sparse_value = ;
    // Onnx__TensorProto* default_value = ;
    // float default_value_float = ;
    // size_t default_n_value_floats = ;
    // float* default_value_floats = ;
    // int64_t default_value_int = ;
    // size_t default_n_value_ints = ;
    // int64_t* default_value_ints = ;
    // char* default_value_string = ;
    // size_t default_n_value_strings = ;
    // char** default_value_strings = ;

    // context_operator__ai_onnx__constant__12 *op_ctx = NULL;
    // op_ctx = malloc(sizeof(context_operator__ai_onnx__constant__12));
    // TRACE_FATAL(0 , !op_ctx, "could not allocate executer_context");

    // op_ctx->sparse_value = a_sparse_value?a_sparse_value->sparse_tensor:default_sparse_value;
    // op_ctx->value = a_value?a_value->t:default_value;
    // op_ctx->value_float = a_value_float?a_value_float->f:default_value_float;
    // op_ctx->n_value_floats = a_value_floats?a_value_floats->n_floats:default_n_value_floats;
    // op_ctx->value_floats = a_value_floats?a_value_floats->floats:ARRAYDUP(default_value_floats,default_n_value_floats);
    // TRACE_FATAL(0, !op_ctx->value_floats, "malloc failed");
    // op_ctx->value_int = a_value_int?a_value_int->i:default_value_int;
    // op_ctx->n_value_ints = a_value_ints?a_value_ints->n_ints:default_n_value_ints;
    // op_ctx->value_ints = a_value_ints?a_value_ints->ints:ARRAYDUP(default_value_ints,default_n_value_ints);
    // TRACE_FATAL(0, !op_ctx->value_ints, "malloc failed");
    // op_ctx->value_string = a_value_string?strndup((char*)a_value_string->s.data, a_value_string->s.len):default_value_string;
    // op_ctx->n_value_strings = a_value_strings?a_value_strings->n_strings:default_n_value_strings;
    // if (a_value_strings) {
    //     op_ctx->value_strings = malloc(a_value_strings->n_strings * sizeof(char*));
    //     TRACE_FATAL(0, !op_ctx->value_strings, "malloc failed");
    //     for (int i = 0; i < a_value_strings->n_strings; i++) { op_ctx->value_strings[i] = strndup((char*)a_value_strings->strings[i].data, a_value_strings->strings[i].len); }
    // } else {
    //     op_ctx->value_strings = a_value_strings?a_value_strings->strings:ARRAYDUP(default_value_strings,default_n_value_strings);
    //     TRACE_FATAL(0, !op_ctx->value_strings, "malloc failed");
    // }

    // TRACE_VAR(2, true, op_ctx->sparse_value, "%p");
    // TRACE_TENSOR(2, true, op_ctx->value);
    // TRACE_VAR(2, true, op_ctx->value_float, "%f");
    // TRACE_ARRAY(2, true, op_ctx->value_floats, , op_ctx->n_value_floats, "%f");
    // TRACE_VAR(2, true, op_ctx->value_int, "%" PRId64);
    // TRACE_ARRAY(2, true, op_ctx->value_ints, , op_ctx->n_value_ints, "%" PRId64);
    // TRACE_VAR(2, true, op_ctx->value_string, "\"%s\"");
    // TRACE_ARRAY(2, true, op_ctx->value_strings, , op_ctx->n_value_strings, "\"%s\"");

    /* INITIALIZE OUTPUTS DATA_TYPE AND SHAPE HERE */

    /* Attention. Don't do this. Leaving it here so I don't waste again
    hours debugging this. Don't modify the memory address that the
    output is pointing. Otherwise the next operator in the node won't
    be able to find it. Copy it instead*/
    //ctx->outputs[0] = value->t;

    TRACE_FATAL(0, !a_value, "value not specified!");

    char *saved_name = o_output->name;
    memcpy(o_output, a_value->t, sizeof(Onnx__TensorProto));
    o_output->name = saved_name;
    convertRawDataOfTensorProto(o_output);

    /* MALLOC OUTPUT TENSORS HERE */

    // mallocTensorData(o_output);

    TRACE_TENSOR(2, true, o_output);

    /* CHOOSE EXECUTER AND CONTEXT HERE */
    /* YOU MAY USE THE GENERATED RESOLVER */

    ctx->executer = &execute_operator__ai_onnx__constant__12;
    // ctx->executer_context = op_ctx;

    TRACE_EXIT(1);

    /* CHANGE RETURN CODE IF THIS PREPARER IS VALID */
    // return OP_ENOSYS;
    return OP_OK;
}