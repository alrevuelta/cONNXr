//this file was generated by ../../../../../../scripts/onnx_generator/OperatorTemplate.py
#include "operator__ai_onnx__conv__11.h"
#include "tracing.h"
#include "utils.h"
#include <string.h>

operator_status
prepare_operator__ai_onnx__conv__11(
    node_context *ctx
)
{
    TRACE_ENTRY(1);

    TRACE_NODE(2, true, ctx->onnx_node);

    /* UNCOMMENT AS NEEDED */

    Onnx__TensorProto *i_X = searchInputByName(ctx, 0);
    Onnx__TensorProto *i_W = searchInputByName(ctx, 1);
    // Onnx__TensorProto *i_B = searchInputByName(ctx, 2);

    TRACE_TENSOR(2, true, i_X);
    TRACE_TENSOR(2, true, i_W);
    // TRACE_TENSOR(2, B, i_B);

    Onnx__AttributeProto *a_auto_pad = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"auto_pad");
    Onnx__AttributeProto *a_dilations = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"dilations");
    Onnx__AttributeProto *a_group = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"group");
    Onnx__AttributeProto *a_kernel_shape = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"kernel_shape");
    Onnx__AttributeProto *a_pads = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"pads");
    Onnx__AttributeProto *a_strides = searchAttributeNyName(ctx->onnx_node->n_attribute,ctx->onnx_node->attribute,"strides");

    TRACE_ATTRIBUTE(2, a_auto_pad, a_auto_pad);
    TRACE_ATTRIBUTE(2, a_dilations, a_dilations);
    TRACE_ATTRIBUTE(2, a_group, a_group);
    TRACE_ATTRIBUTE(2, a_kernel_shape, a_kernel_shape);
    TRACE_ATTRIBUTE(2, a_pads, a_pads);
    TRACE_ATTRIBUTE(2, a_strides, a_strides);

    Onnx__TensorProto *o_Y = searchOutputByName(ctx, 0);

    /* ALLOCATE AND INITIALIZE CONTEXT HERE IF NEEDED */

    // X : (B x C x D1 x D2 x ... )
    int64_t B   = i_X->dims[0];
    // int64_t C   = i_X->dims[1];
    int64_t n_D = i_X->n_dims-2;
    int64_t *D  = &i_X->dims[2];

    //TODO is the case of t_X->n_dims < 4 even valid for this operator?
    //TODO or is this a dirty hack for one of the models? :D
    if (i_X->n_dims == 2) {
        B   = 1;
        // C   = 1;
        n_D = i_X->n_dims;
        D   = i_X->dims;
    }

    // W : (M x C/group x K1 x K2 x ... )
    int64_t M  = i_W->dims[0];
    // int64_t group = (C + i_W->dims[1] -1) / i_W->dims[1];
    // int64_t n_K = i_W->n_dims-2;
    // int64_t *K  = &i_W->dims[2];

    char* default_auto_pad = "NOTSET";
    int64_t default_n_dilations = n_D;
    int64_t default_dilations = 1;
    int64_t default_group = 1;
    size_t default_n_kernel_shape = i_W->n_dims;
    int64_t* default_kernel_shape = i_W->dims;
    size_t default_n_pads = n_D;
    // int64_t default_pads_begin = 0;
    // int64_t default_pads_end = 0;
    size_t default_n_strides = n_D;
    int64_t default_strides = 1;

    context_operator__ai_onnx__conv__11 *op_ctx = NULL;
    op_ctx = malloc(sizeof(context_operator__ai_onnx__conv__11));
    TRACE_FATAL(0 , !op_ctx, "could not allocate executer_context");

    op_ctx->auto_pad = a_auto_pad?strndup((char*)a_auto_pad->s.data, a_auto_pad->s.len):default_auto_pad;

    op_ctx->n_dilations = a_dilations?a_dilations->n_ints:default_n_dilations;
    op_ctx->dilations = malloc(op_ctx->n_dilations * sizeof(int64_t));
    TRACE_FATAL(0, !op_ctx->dilations, "malloc failed");
    for(int i = 0; i < op_ctx->n_dilations; i++) {
        op_ctx->dilations[i] = a_dilations?a_dilations->ints[i]:default_dilations;
    }

    op_ctx->n_strides = a_strides?a_strides->n_ints:default_n_strides;
    op_ctx->strides = malloc(op_ctx->n_strides * sizeof(int64_t));
    TRACE_FATAL(0, !op_ctx->strides, "malloc failed");
    for(int i = 0; i < op_ctx->n_strides; i++) {
        op_ctx->strides[i] = a_strides?a_strides->ints[i]:default_strides;
    }

    op_ctx->group = a_group?a_group->i:default_group;

    op_ctx->n_kernel_shape = a_kernel_shape?a_kernel_shape->n_ints:default_n_kernel_shape;
    op_ctx->kernel_shape = a_kernel_shape?a_kernel_shape->ints:default_kernel_shape;

    op_ctx->n_pads = a_pads?a_pads->n_ints/2:default_n_pads;
    op_ctx->pads_begin = malloc(op_ctx->n_pads * sizeof(int64_t));
    TRACE_FATAL(0, !op_ctx->pads_begin, "malloc failed");
    op_ctx->pads_end   = malloc(op_ctx->n_pads * sizeof(int64_t));
    TRACE_FATAL(0, !op_ctx->pads_end, "malloc failed");
    for(int i = 0; i < op_ctx->n_pads; i++) {
        if (a_pads && i < a_pads->n_ints/2) {
            op_ctx->pads_begin[i] = a_pads->ints[i];
            op_ctx->pads_end[i]   = a_pads->ints[a_pads->n_ints/2 + i];
            TRACE_WARN(0, strcmp(op_ctx->auto_pad, "NOTSET") != 0, "auto_pad and explicit padding specified for dimension %d: using explicit", i+2);
            continue;
        }
        if (strcmp(op_ctx->auto_pad, "VALID") == 0) {
            op_ctx->pads_begin[i] = 0;
            op_ctx->pads_end[i]   = 0;
            continue;
        }

        // output dimensions should match input dimensions
        // we need to calculate padding accordingly
        // strange behaviour regarding strides
        // should be
        // int64_t unstrided = i_X->dims[2+i] * op_ctx->strides[i];
        // but according to
        // https://github.com/Microsoft/onnxruntime/issues/495
        // https://github.com/onnx/onnx/issues/1921
        // and the onnx test of maxpool it is
        // int64_t unstrided = D[i];
        // if conv fails, look this up again
        int64_t unstrided = D[i] * op_ctx->strides[i];
        int64_t input     = unstrided + (op_ctx->kernel_shape[i] - 1);
        int64_t pads      = input - D[i];
        if (strcmp(op_ctx->auto_pad, "SAME_UPPER") == 0) {
            op_ctx->pads_begin[i] = pads / 2;
            op_ctx->pads_end[i]   = pads - op_ctx->pads_begin[i];
            continue;
        }
        if (strcmp(op_ctx->auto_pad, "SAME_LOWER") == 0) {
            op_ctx->pads_end[i]   = pads / 2;
            op_ctx->pads_begin[i] = pads - op_ctx->pads_end[i];
            continue;
        }
        TRACE_WARN(0, strcmp(op_ctx->auto_pad, "NOTSET") != 0, "no valid padding specified for dimension %d: zeroing padding", i+2);
        op_ctx->pads_begin[i] = 0;
        op_ctx->pads_end[i]   = 0;
    }

    TRACE_VAR(2, true, op_ctx->auto_pad, "\"%s\"");
    TRACE_ARRAY(2, true, op_ctx->dilations, , op_ctx->n_dilations, "%" PRId64);
    TRACE_VAR(2, true, op_ctx->group, "%" PRId64);
    TRACE_ARRAY(2, true, op_ctx->kernel_shape, , op_ctx->n_kernel_shape, "%" PRId64);
    TRACE_ARRAY(2, true, op_ctx->pads_begin, , op_ctx->n_pads, "%" PRId64);
    TRACE_ARRAY(2, true, op_ctx->pads_end, , op_ctx->n_pads, "%" PRId64);
    TRACE_ARRAY(2, true, op_ctx->strides, , op_ctx->n_strides, "%" PRId64);

    /* INITIALIZE OUTPUTS DATA_TYPE AND SHAPE HERE */

    o_Y->n_dims = i_W->n_dims;
    o_Y->dims   = malloc(o_Y->n_dims * sizeof(int64_t));

    // strange dimension calculation regarding
    // https://github.com/Microsoft/onnxruntime/issues/495
    // https://github.com/onnx/onnx/issues/1921
    // if conv fails, look this up again
    o_Y->dims[0] = B;
    o_Y->dims[1] = M;
    for(int i = 0; i < n_D; i++) {
        int64_t padded  = op_ctx->pads_begin[i] + D[i] + op_ctx->pads_end[i];
        int64_t output  = padded - (op_ctx->kernel_shape[i] - 1);
        int64_t strided = (output + op_ctx->strides[i] -1) / op_ctx->strides[i];
        o_Y->dims[2+i] = strided;
    }

    o_Y->has_raw_data = 0;
    o_Y->data_type    = i_X->data_type;

    /* MALLOC OUTPUT TENSORS HERE */

    mallocTensorData(o_Y);

    TRACE_TENSOR(2, true, o_Y);

    /* CHOOSE EXECUTER AND CONTEXT HERE */
    /* YOU MAY USE THE GENERATED RESOLVER */

    ctx->executer = resolve_operator__ai_onnx__conv__11(ctx);
    ctx->executer_context = op_ctx;

    TRACE_EXIT(1);

    /* CHANGE RETURN CODE IF THIS PREPARER IS VALID */
    // return OP_ENOSYS;
    return OP_OK;
}