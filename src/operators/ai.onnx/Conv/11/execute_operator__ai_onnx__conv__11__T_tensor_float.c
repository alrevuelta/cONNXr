//this file was generated by ../../../../../../scripts/onnx_generator/OperatorTemplate.py
#include "operator__ai_onnx__conv__11.h"
#include "tracing.h"
#include "utils.h"
#include "index.h"

operator_status
execute_operator__ai_onnx__conv__11__T_tensor_float(
    node_context *ctx
)
{
    TRACE_ENTRY(1);

    TRACE_NODE(2, true, ctx->onnx_node);

    /* UNCOMMENT AS NEEDED */

    Onnx__TensorProto *i_X = searchInputByName(ctx, 0);
    Onnx__TensorProto *i_W = searchInputByName(ctx, 1);
    Onnx__TensorProto *i_B = searchInputByName(ctx, 2);

    TRACE_TENSOR(2, true, i_X);
    TRACE_TENSOR(2, true, i_W);
    TRACE_TENSOR(2, i_B, i_B);

    context_operator__ai_onnx__conv__11 *op_ctx = ctx->executer_context;

    __attribute__((unused))
    char* auto_pad = op_ctx->auto_pad;
    __attribute__((unused))
    size_t n_dilations = op_ctx->n_dilations;
    __attribute__((unused))
    int64_t* dilations = op_ctx->dilations;
    __attribute__((unused))
    int64_t group = op_ctx->group;
    __attribute__((unused))
    size_t n_kernel_shape = op_ctx->n_kernel_shape;
    __attribute__((unused))
    int64_t* kernel_shape = op_ctx->kernel_shape;
    __attribute__((unused))
    size_t n_pads = op_ctx->n_pads;
    __attribute__((unused))
    int64_t* pads_begin = op_ctx->pads_begin;
    __attribute__((unused))
    int64_t* pads_end = op_ctx->pads_end;
    __attribute__((unused))
    size_t n_strides = op_ctx->n_strides;
    __attribute__((unused))
    int64_t* strides = op_ctx->strides;

    TRACE_VAR(2, true, auto_pad, "\"%s\"");
    TRACE_ARRAY(2, true, dilations, , n_dilations, "%" PRId64);
    TRACE_VAR(2, true, group, "%" PRId64);
    TRACE_ARRAY(2, true, kernel_shape, , n_kernel_shape, "%" PRId64);
    TRACE_ARRAY(2, true, pads_begin, , n_pads, "%" PRId64);
    TRACE_ARRAY(2, true, pads_end, , n_pads, "%" PRId64);
    TRACE_ARRAY(2, true, strides, , n_strides, "%" PRId64);

    Onnx__TensorProto *o_Y = searchOutputByName(ctx, 0);

    TRACE_TENSOR(2, true, o_Y);

    /* DO CALCULATION HERE */


  if (i_X->n_dims == 4 || i_X->n_dims == 2) {
    //legacy code, almost untouched, fast for 2 and 4 dimension
    //TODO replace, refactor
    int64_t h_kernel = kernel_shape[0];
    int64_t w_kernel = kernel_shape[1];
    __attribute__((unused))
    int64_t d_kernel = kernel_shape[2];
    int64_t h_stride = strides[0];
    int64_t w_stride = strides[1];
    int64_t h_dilation = dilations[0];
    int64_t w_dilation = dilations[1];
    int64_t h_pad    = pads_begin[0];
    int64_t w_pad    = pads_begin[1];
    int b, i, j, k, m, n, d;
    for(b = 0; b < o_Y->dims[0]; ++b){
      for(k = 0; k < o_Y->dims[1]; ++k){
        int g = (k/(o_Y->dims[1]/group));
        TRACE_BOUND_FATAL(3, true, g, 0, (int)group, "%d");
        for(i = 0; i < o_Y->dims[2]; ++i){
          for(j = 0; j < o_Y->dims[3]; ++j){
            // TODO replace all this calculations by macros?
            uint64_t out_index = j + o_Y->dims[3]*(i + o_Y->dims[2]*(k + o_Y->dims[1]*b));
            float value = 0;

            if (i_X->n_dims == 4){
              const int offset_in = g*(i_X->dims[3]*i_X->dims[2]*(i_X->dims[1]/group));
              TRACE_VAR(4, true, offset_in, "%d");
              for(d = 0; d < i_W->dims[1]; ++d){
                for(n = 0; n < h_kernel; ++n){   // TODO use i_W->dims[2] instead?
                  for(m = 0; m < w_kernel; ++m){ // TODO use i_W->dims[3] instead?
                    int cur_h = i*h_stride + n * h_dilation - h_pad;
                    int cur_w = j*w_stride + m * w_dilation - w_pad;

                    /* This is hardcoded to make it work with mnist model, where
                    the input is 1x1x28x28 */
                    int index = offset_in + cur_w + i_X->dims[3]*(cur_h + i_X->dims[2]*(d + 0*i_X->dims[1]));
                    //TRACE_LEVEL0("%d, %d, %d index=%d\n", d, cur_h, cur_w, index);

                    int valid = (cur_h >= 0 && cur_h < i_X->dims[2] &&
                                 cur_w >= 0 && cur_w < i_X->dims[3]);
                    TRACE_BOUND_FATAL(5, valid, index, 0, (int)i_X->n_float_data, "%d");
                    float val = (valid != 0) ? i_X->float_data[index] : 0;
                    int index_kernel = k*i_W->dims[3]*i_W->dims[2]*i_W->dims[1] + d*i_W->dims[3]*i_W->dims[2] + n*h_kernel + m; // change h_kernel by i_W->dims[x]
                    value += val * i_W->float_data[index_kernel];
                    //TRACE_LEVEL0("%fx%f+\n", val, i_W->float_data[index_kernel]);
                  }
                }
              }
            }else if (i_X->n_dims == 2){
              const int offset_in = g*(i_X->dims[1]/group);
              TRACE_VAR(4, true, offset_in, "%d");
              for(n = 0; n < h_kernel; ++n){
                for(m = 0; m < w_kernel; ++m){
                  int cur_h = i*h_stride + n * h_dilation - h_pad;
                  int cur_w = j*w_stride + m * w_dilation - w_pad;
                  //printf("%d, %d\n", cur_h, cur_w);
                  int index = offset_in + cur_w + i_X->dims[1]*cur_h;
                  //printf("index=%d\n", index);
                  int valid = (cur_h >= 0 && cur_h < i_X->dims[0] &&
                               cur_w >= 0 && cur_w < i_X->dims[1]);
                  TRACE_BOUND_FATAL(5, valid, index, 0, (int)i_X->n_float_data, "%d");
                  float val = (valid != 0) ? i_X->float_data[index] : 0;
                  int index_kernel = k*i_W->dims[3]*i_W->dims[2]*i_W->dims[1] + n*h_kernel + m;
                  value += val * i_W->float_data[index_kernel];
                  //TRACE_LEVEL0("%fx%f+\n", val, input[1]->float_data[index_kernel]);
                }
              }
            }else{
              /* TODO */
            }
            TRACE_BOUND_FATAL(4, true, (size_t)out_index, (size_t)0, o_Y->n_float_data, "%zu");
            o_Y->float_data[out_index] = value;
            //printf("%lld\n", out_index);
            //printf("[%lld]=%f\n", out_index, value);

            /* TODO This is a huge crap to make it work with tinyYOLO
            It adds the bias, but this if will waste a lot of time. Make
            this nice!
            */
            if (i_B != NULL){
              o_Y->float_data[out_index] += i_B->float_data[k];
            }
          }
        }
      }
    }
    TRACE_EXIT(1);
    return 0;
  }

  TRACE_WARN(0, true, "running generic implementation...slow!");

  // X : (B x C x D1 x D2 x ... )
  __attribute__((unused))
  int64_t B   = i_X->dims[0];
  int64_t C   = i_X->dims[1];

  // W : (M x C/group x K1 x K2 x ... )
  int64_t M  = i_W->dims[0];
  int64_t n_K = i_W->n_dims-2;

  index_ctx kernel_index;
  int64_t   kernel_indices[i_W->n_dims];
  int64_t   kernel_offsets[i_W->n_dims];
  index_init(&kernel_index, i_W->n_dims, i_W->dims, kernel_indices, kernel_offsets);

  index_ctx input_index;
  int64_t   input_indices[i_X->n_dims];
  int64_t   input_offsets[i_X->n_dims];
  index_init(&input_index, i_X->n_dims, i_X->dims, input_indices, input_offsets);

  index_ctx output_index;
  int64_t   output_indices[o_Y->n_dims];
  int64_t   output_offsets[o_Y->n_dims];
  index_init(&output_index, o_Y->n_dims, o_Y->dims, output_indices, output_offsets);

  do {
    TRACE_INDEX(3, true, &output_index);
    float value = i_B?i_B->float_data[index_get(&output_index, 1)]:0;
    int64_t input_ch_offset = index_get(&output_index, 1)/(M/group);
    TRACE_VAR(3, true, input_ch_offset, "%" PRId64);
    index_set(&kernel_index, 0, index_get(&output_index, 1));
    index_set(&input_index, 0, index_get(&output_index, 0));
    for (int64_t ch = 0; ch < C/group; ch++) {
      TRACE_BOUND(4, true, ch, (int64_t)0, C, "%" PRId64);
      index_set(&kernel_index, 1, ch);
      index_set(&input_index, 1, ch + input_ch_offset);
      index_reset_sub(&kernel_index, 2);
      do {
        TRACE_INDEX(5, true, &kernel_index);
        bool is_padded = false;
        for (int i = 0; i < n_K; i++) {
          int64_t input = 0;
          input += index_get(&output_index, 2+i) * strides[i];
          input += index_get(&kernel_index, 2+i) * dilations[i];
          TRACE(6, true, "dim %d index %" PRId64 " valid in %" PRId64 ":%" PRId64 , 2+i, input, pads_begin[i], i_X->dims[2+i]);
          if ( input < pads_begin[i] || input - pads_begin[i] >= i_X->dims[2+i]  ) {
            is_padded = true;
            TRACE(6, true, "dim %d index %" PRId64 " is padded, skipping", 2+i, input);
            break;
          }
          index_set(&input_index, 2+i, input - pads_begin[i]);
        }
        if (is_padded) {
          continue;
        }
        TRACE_INDEX(5, true, &input_index);
        float   data_input = i_X->float_data[input_index.offset];
        float   data_kernel = i_W->float_data[kernel_index.offset];
        TRACE_VAR(5, true, data_input, "%f");
        TRACE_VAR(5, true, data_kernel, "%f");
        value +=  data_input * data_kernel;
        TRACE_VAR(5, true, value, "%f");
      } while(index_inc_sub(&kernel_index, 1));
    }
    TRACE(3, true, "writing value: %f @ %" PRId64, value, output_index.offset);
    o_Y->float_data[output_index.offset] = value;
  } while (index_inc(&output_index));

    TRACE_EXIT(1);

    /* CHANGE RETURN CODE IF THIS EXECUTER IS VALID */
    // return OP_ENOSYS;
    return OP_OK;
}