//this file was generated by ../../../../../../scripts/onnx_generator/OperatorHeader.py
# ifndef OPERATOR_OPERATOR__AI_ONNX__CONV__11_H
# define OPERATOR_OPERATOR__AI_ONNX__CONV__11_H

# include "operators/operator.h"
# include "operators/operator_stub.h"
# include "operators/operator_info.h"

/**
 * ai.onnx operator 'Conv' version 11
 *
 * @param[in]  ctx  Operator context
 * @return          Status code
 *
 * The convolution operator consumes an input tensor and a filter, and
 * computes the output.
 *
 * Constraint T:
 *   Constrain input and output types to float tensors.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Input T X:
 *   Input data tensor from previous layer; has size (N x C x H x W), where N
 *   is the batch size, C is the number of channels, and H and W are the height
 *   and width. Note that this is for the 2D image. Otherwise the size is (N x
 *   C x D1 x D2 ... x Dn). Optionally, if dimension denotation is in effect,
 *   the operation expects input data tensor to arrive with the dimension
 *   denotation of [DATA_BATCH, DATA_CHANNEL, DATA_FEATURE, DATA_FEATURE ...].
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 *
 * Input T W:
 *   The weight tensor that will be used in the convolutions; has size (M x
 *   C/group x kH x kW), where C is the number of channels, and kH and kW are
 *   the height and width of the kernel, and M is the number of feature maps.
 *   For more than 2 dimensions, the kernel shape will be (M x C/group x k1 x
 *   k2 x ... x kn), where (k1 x k2 x ... kn) is the dimension of the kernel.
 *   Optionally, if dimension denotation is in effect, the operation expects
 *   the weight tensor to arrive with the dimension denotation of
 *   [FILTER_OUT_CHANNEL, FILTER_IN_CHANNEL, FILTER_SPATIAL, FILTER_SPATIAL
 *   ...]. X.shape[1] == (W.shape[1] * group) == C (assuming zero based indices
 *   for the shape array). Or in other words FILTER_IN_CHANNEL should be equal
 *   to DATA_CHANNEL.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 *
 * Input T B:
 *   Optional 1D bias to be added to the convolution, has size of M.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Output T Y:
 *   Output data tensor that contains the result of the convolution. The
 *   output dimensions are functions of the kernel size, stride size, and pad
 *   lengths.
 *   Allowed Types: tensor_double, tensor_float, tensor_float16
 * Attribute STRING auto_pad (optional):
 *   auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where
 *   default value is NOTSET, which means explicit padding is used. SAME_UPPER
 *   or SAME_LOWER mean pad the input so that the output spatial size match the
 *   input.In case of odd number add the extra padding at the end for
 *   SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding.
 *
 * Attribute INTS dilations (optional):
 *   dilation value along each spatial axis of the filter. If not present, the
 *   dilation defaults is 1 along each spatial axis.
 *
 * Attribute INT group (optional):
 *   number of groups input channels and output channels are divided into.
 *
 * Attribute INTS kernel_shape (optional):
 *   The shape of the convolution kernel. If not present, should be inferred
 *   from input W.
 *
 * Attribute INTS pads (optional):
 *   Padding for the beginning and ending along each spatial axis, it can take
 *   any value greater than or equal to 0. The value represent the number of
 *   pixels added to the beginning and end part of the corresponding axis.
 *   `pads` format should be as follow [x1_begin, x2_begin...x1_end,
 *   x2_end,...], where xi_begin the number of pixels added at the beginning of
 *   axis `i` and xi_end, the number of pixels added at the end of axis `i`.
 *   This attribute cannot be used simultaneously with auto_pad attribute. If
 *   not present, the padding defaults to 0 along start and end of each spatial
 *   axis.
 *
 * Attribute INTS strides (optional):
 *   Stride along each spatial axis. If not present, the stride defaults is 1
 *   along each spatial axis.
 *
 * @since version 11
 *
 * @see io/onnx/onnx/defs/nn/defs.cc:793
 * @see https://github.com/onnx/onnx/blob/master/docs/Operators.md#Conv
 */

operator_status
prepare_operator__ai_onnx__conv__11(
    node_context *ctx
);

extern operator_info info_operator__ai_onnx__conv__11;

typedef struct {
    char* auto_pad;
    size_t n_dilations;
    int64_t* dilations;
    int64_t group;
    size_t n_kernel_shape;
    int64_t* kernel_shape;
    size_t n_pads;
    int64_t* pads_begin;
    int64_t* pads_end;
    size_t n_strides;
    int64_t* strides;

} context_operator__ai_onnx__conv__11;

operator_executer
resolve_operator__ai_onnx__conv__11(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__conv__11__T_tensor_double(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__conv__11__T_tensor_float(
    node_context *ctx
);

operator_status
execute_operator__ai_onnx__conv__11__T_tensor_float16(
    node_context *ctx
);

# endif